{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sphere with few parameters we'll have voxels with way more parameters.\n",
    "our sphere model isnt complex enough to fit to enough scenes\n",
    "\n",
    "Therefore we'll move to voxels to represent more scenes, and finally we'll move to Neural Networks that can represent even more complex scenes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Camera / Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instrinsic: intrinsic paramters eg. the camera parameters the orientation, focal length etc.\n",
    "for NeRF we make sure the intrinsic parameters like focal length are the same for input image while the position is changing overtime.\n",
    "\n",
    "- do not use autofocus on your phone use the same focal length for all images.\n",
    "- most nerf code assumes that intrinsic params between input images remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(datapath, mode='train'):\n",
    "    pose_file_names = [f for f in os.listdir(datapath + '/{mode}/pose/') if f.endswith('.txt')]\n",
    "    intrinsics_file_names= [f for f in os.listdir(datapath + '/{mode}/intrinsics/') if f.endswith('.txt')]\n",
    "        \n",
    "    img_file_names = [f for f in os.listdir(datapath + '/imgs/') if 'train' in f]\n",
    "    print(f\"Image files length: {len(img_file_names)}\")\n",
    "    print(f\"Pose files length: {len(pose_file_names)}\")\n",
    "    print(f\"Intrinsic files length: {len(intrinsics_file_names)}\")\n",
    "    \n",
    "    assert len(pose_file_names) == len(intrinsics_file_names)\n",
    "    assert len(img_file_names) == len(pose_file_names)\n",
    "    \n",
    "    #Read\n",
    "    N = len(pose_file_names)\n",
    "    poses = np.zeros((N, 4, 4)) #N 4x4 matrixes (homogeneous)\n",
    "    intrinsics = np.zeros((N, 4, 4))\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        pose_name = pose_file_names[i]\n",
    "        pose = open(datapath + '{mode}/pose/' + pose_name).read().split()\n",
    "        poses[i] = np.array(pose, dtype=float).reshape(4,4)\n",
    "        \n",
    "        # print(poses[i])\n",
    "        \n",
    "        intrinsics_name = intrinsics_file_names[i]\n",
    "        intrinsic = open(datapath + '{mode}/intrinsics/' + intrinsics_name).read().split()\n",
    "        intrinsics[i] = np.array(intrinsic, dtype=float).reshape(4, 4)\n",
    "        \n",
    "        # print(intrinsics[i])\n",
    "        \n",
    "        #Read images\n",
    "        image_name = img_file_names[i]\n",
    "        img = imageio.imread(datapath + '/imgs/' + image_name)\n",
    "        max_img_intensity = float(img.max()) #255 \n",
    "        img = img / max_img_intensity #normalizing pixel intensities so theyre between 0-1\n",
    "        \n",
    "        images.append(img[None, ...]) #unsqueeze 1st dim in numpy\n",
    "        \n",
    "    print(f\"Image size: {img.shape}\")   \n",
    "    images = np.concatenate(images)\n",
    "    print(images.shape)\n",
    "    \n",
    "    H = images.shape[1]\n",
    "    W = images.shape[2]\n",
    "    \n",
    "    # remove the 4th dimension to get rid of the alpha channel ie opacity\n",
    "    if images.shape[3] == 4: #RGBA -> RGB\n",
    "        images = images[..., :3] * images[..., -1:] + (1-images[..., -1:])\n",
    "    \n",
    "    # plt.imshow(images[0])\n",
    "    # plt.show()\n",
    "    \n",
    "    rays_origin = np.zeros((N, H*W, 3))\n",
    "    rays_direction = np.zeros((N, H*W, 3))\n",
    "    target_px_values = images.reshape((N, H*W, 3))\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        camera2world = poses[i]\n",
    "        f = intrinsics[i,0,0]\n",
    "        \n",
    "        u = np.arange(W)\n",
    "        v = np.arange(H)\n",
    "        u, v = np.meshgrid(u, v)\n",
    "\n",
    "        dirs = np.stack((u - W / 2,\n",
    "                        -(v - H / 2),\n",
    "                        - np.ones_like(u) * f), axis=-1)\n",
    "\n",
    "        dirs = (camera2world[:3, :3] @ dirs[..., None]).squeeze(-1)\n",
    "        dirs = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)\n",
    "        \n",
    "        rays_direction[i] = dirs.reshape(-1, 3)\n",
    "        rays_origin[i] += camera2world[:3, 3]\n",
    "        \n",
    "    return rays_origin, rays_direction, target_px_values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 160000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "origin, direction, target_px_values = dataset.get_rays('fox/fox/',mode='train')\n",
    "\n",
    "dataloader = DataLoader(torch.cat((torch.from_numpy(origin),\n",
    "                                   torch.from_numpy(direction),\n",
    "                                   torch.from_numpy(target_px_values)), dim=1),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_origin, test_direction, test_target_px_values = dataset.get_rays('fox/fox/', mode='test')\n",
    "\n",
    "\n",
    "print(f\"origin shape: {origin.shape}\")\n",
    "print(f\"direction shape: {direction.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
